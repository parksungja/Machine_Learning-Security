{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3811629",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. 시작 - 스펨메일"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "75da8d6d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Error loading stopwords: <urlopen error [SSL:\n",
      "[nltk_data]     CERTIFICATE_VERIFY_FAILED] certificate verify failed:\n",
      "[nltk_data]     unable to get local issuer certificate (_ssl.c:1000)>\n"
     ]
    }
   ],
   "source": [
    "# 데이터와 레이블을 불러오고 전처리를 수행\n",
    "import string\n",
    "import email\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "\n",
    "punctuations = list(string.punctuation)\n",
    "stopwords = set(nltk.corpus.stopwords.words('english'))\n",
    "stemmer = nltk.PorterStemmer()\n",
    "\n",
    "# 이메일의 여러 부분을 하나의 문자열로 합한다.\n",
    "def flatten_to_string(parts):\n",
    "    ret = []\n",
    "    if type(parts) == str:\n",
    "        ret.append(parts)\n",
    "    elif type(parts) == list:\n",
    "        for part in parts:\n",
    "            ret += flatten_to_string(part)\n",
    "    elif parts.get_content_type == 'text/plain':\n",
    "        ret += parts.get_payload()\n",
    "    return ret\n",
    "\n",
    "# 이메일로부터 제목과 내용 텍스트를 추출한다.\n",
    "def extract_email_text(path):\n",
    "    # 압력 파일로부터 하나의 이메일을 불러온다.\n",
    "    with open(path, errors='ignore') as f:\n",
    "        msg = email.message_from_file(f)\n",
    "    if not msg:\n",
    "        return \"\"\n",
    "\n",
    "    # 이메일 제목을 불러온다.\n",
    "    subject = msg['Subject']\n",
    "    if not subject:\n",
    "        subject = \"\"\n",
    "\n",
    "    # 이메일 내용을 불러온다.\n",
    "    body = ' '.join(m for m in flatten_to_string(msg.get_payload()) if type(m) == str)\n",
    "    if not body:\n",
    "        body = \"\"\n",
    "\n",
    "    return subject + ' ' + body\n",
    "\n",
    "# 이메일을 형태소 분석한다.\n",
    "def load(path):\n",
    "    email_text = extract_email_text(path)\n",
    "    if not email_text:\n",
    "        return []\n",
    "\n",
    "    # 메시지를 토큰화한다.\n",
    "    tokens = nltk.word_tokenize(email_text)\n",
    "\n",
    "    # 토큰에서 마침표를 제거한다.\n",
    "    tokens = [i.strip(\"\".join(punctuations)) for i in tokens if i not in punctuations]\n",
    "\n",
    "    # 자주 사용하지 않는 단어를 제거한다.\n",
    "    if len(tokens) > 2:\n",
    "        return [stemmer.stem(w) for w in tokens if w not in stopwords]\n",
    "    return []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c5e03f93",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Error loading punkt_tab: <urlopen error [SSL:\n",
      "[nltk_data]     CERTIFICATE_VERIFY_FAILED] certificate verify failed:\n",
      "[nltk_data]     unable to get local issuer certificate (_ssl.c:1000)>\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "nltk.download('punkt_tab')\n",
    "\n",
    "DATA_DIR = './trec07p/data/'\n",
    "LABELS_FILE = './trec07p/full/index'\n",
    "TRAINING_SET_RATIO = 0.7\n",
    "\n",
    "labels = {}\n",
    "spam_words = set()\n",
    "ham_words = set()\n",
    "\n",
    "# Read the labels\n",
    "with open(LABELS_FILE) as f:\n",
    "    for line in f:\n",
    "        line = line.strip()\n",
    "        label, key = line.split()\n",
    "        labels[key.split('/')[-1]] = 1 if label.lower() == 'ham' else 0\n",
    "\n",
    "# Split corpus into train and test sets\n",
    "filelist = os.listdir(DATA_DIR)\n",
    "X_train = filelist[:int(len(filelist)*TRAINING_SET_RATIO)]\n",
    "X_test = filelist[int(len(filelist)*TRAINING_SET_RATIO):]\n",
    "\n",
    "for filename in X_train:\n",
    "        path = os.path.join(DATA_DIR, filename)\n",
    "        if filename in labels:\n",
    "            label = labels[filename]\n",
    "            stems = load(path)\n",
    "            if not stems:\n",
    "                continue\n",
    "            if label == 1:\n",
    "                ham_words.update(stems)\n",
    "            elif label == 0:\n",
    "                spam_words.update(stems)\n",
    "            else:\n",
    "                continue\n",
    "\n",
    "blacklist = spam_words - ham_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d604aa8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasketch import MinHash, MinHashLSH\n",
    "\n",
    "# 스팸 파일만 추출한 뒤 LSH 매처(matcher)에\n",
    "spam_files = [x for x in X_train if labels[x] == 0]\n",
    "\n",
    "# MinHashLSH 매처를 자카드 유사도 모드로 설정한다. \n",
    "# MinhashLSH 임계치를 0.5로, 순열 수를 128개로 설정한다.\n",
    "lsh = MinHashLSH(threshold=0.5, num_perm=128)\n",
    "\n",
    "# 스팸 메일의 MinHash를 학습할 때 사용할 LSH 매처를 전달한다.\n",
    "for idx, f in enumerate(spam_files):\n",
    "    minhash = MinHash(num_perm=128)\n",
    "    stems = load(os.path.join(DATA_DIR, f))\n",
    "    if len(stems) < 2: continue\n",
    "    for s in stems:\n",
    "        minhash.update(s.encode('utf-8'))\n",
    "    lsh.insert(f, minhash)\n",
    "\n",
    "def lsh_predict_label(stems):\n",
    "    '''\n",
    "    LSH 매처에 쿼리하는 경우의 반환 값:\n",
    "        0 : 스팸으로 예측\n",
    "        1 : 햄으로 예측\n",
    "       -1 : 파싱 에러\n",
    "    '''\n",
    "    minhash = MinHash(num_perm=128)\n",
    "    if len(stems) < 2:\n",
    "        return -1\n",
    "    for s in stems:\n",
    "        minhash.update(s.encode('utf-8'))\n",
    "    matches = lsh.query(minhash)\n",
    "    if matches:\n",
    "        return 0\n",
    "    else:\n",
    "        return 1\n",
    "\n",
    "def read_email_files():\n",
    "    X = []\n",
    "    y = [] \n",
    "    for i in range(len(labels)):\n",
    "        filename = 'inmail.' + str(i+1)\n",
    "        email_str = extract_email_text(\n",
    "            os.path.join(DATA_DIR, filename))\n",
    "        X.append(email_str)\n",
    "        y.append(labels[filename])\n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "16aac2d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split \n",
    "\n",
    "X, y = read_email_files()\n",
    "\n",
    "X_train, X_test, y_train, y_test, idx_train, idx_test = \\\n",
    "    train_test_split(X, y, range(len(y)), \n",
    "    train_size=TRAINING_SET_RATIO, random_state=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6354a74d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "vectorizer = CountVectorizer()\n",
    "X_train_vector = vectorizer.fit_transform(X_train)\n",
    "X_test_vector = vectorizer.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "017cf614",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification accuracy 95.6%\n"
     ]
    }
   ],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Initialize the classifier and make label predictions\n",
    "mnb = MultinomialNB()\n",
    "mnb.fit(X_train_vector, y_train)\n",
    "y_pred = mnb.predict(X_test_vector)\n",
    "\n",
    "# Print results\n",
    "print('Classification accuracy {:.1%}'.format(accuracy_score(y_test, y_pred)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99ea65a9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
